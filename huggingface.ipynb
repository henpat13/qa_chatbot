{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import ast\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PROMPT_TEMPLATE = \"\"\"\n",
    "Context Information:\n",
    "{context}\n",
    "\n",
    "Table Information:\n",
    "{table}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Your task:\n",
    "1. Carefully analyse the question and the context\n",
    "2. Read through the table information in depth\n",
    "3. Solve the question with clear, step-by-step reasoning\n",
    "4. Provide your solution in the following format as a json dictionary:\n",
    "\n",
    "{{\"Reasoning_Steps\": A detailed explanation of how you approached the problem,\n",
    "\"Relevant_Data_Points\": Key numbers and data you used to calculate the answer,\n",
    "\"Calculation_Formula\": The mathematical formula or logic used to calculate the answer,\n",
    "\"Potential_Validation_Checks\": Ways to verify the answer,\n",
    "\"Final_Answer\": State your answer which should be a number\n",
    "\"Confidence_Level\": Estimate your confidence in the answer from 0-100%\n",
    "}}\n",
    "\n",
    "Be EXPLICIT about your process and any assumptions you made.\n",
    "\"\"\"\n",
    "\n",
    "VALIDATION_PROMPT = \"\"\" \n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Reasoning:\n",
    "{reasoning}\n",
    "\n",
    "Actual Answer:\n",
    "{actual_answer}\n",
    "\n",
    "Your task:\n",
    "1. Assess the question and the reasoning for the calculated answer\n",
    "2. Compare the reasoning with the actual answer\n",
    "3. Identify any potential errors\n",
    "4. Suggest improvements or different approaches\n",
    "5. Provide an overall assessment in the following format as a json dictionary:\n",
    "\n",
    "{{\"Validity_Assessment\": High/Medium/Low,\n",
    "\"Potential_Issues\": List any problems,\n",
    "\"Suggested_Improvements\": Recommendations for improvements,\n",
    "\"Confidence_Level\": Estimate your confidence in your validation from 0-100%}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialQAAgent:\n",
    "    def __init__(self, llm):\n",
    "        \"\"\"\n",
    "        Initialise the Finanical Question Answering Agent\n",
    "\n",
    "        Args:\n",
    "            llm (_type_): the language model used to solve the questions\n",
    "        \"\"\"\n",
    "        \n",
    "        self.llm = llm\n",
    "        self.question_prompt = PromptTemplate(input_variables = ['question', 'context', 'table'], \n",
    "                                              template=QUESTION_PROMPT_TEMPLATE)\n",
    "        self.validate_prompt = PromptTemplate(input_variables=['question', 'reasoning', 'actual_answer'],\n",
    "                                              template=VALIDATION_PROMPT)\n",
    "        \n",
    "    def extract_table_data(self, sample: Dict[str, Any]) -> str:\n",
    "        \"\"\"\n",
    "        Convert table data to a string representation\n",
    "        \n",
    "        Args:\n",
    "            sample: A singular entry from the dataset\n",
    "        \n",
    "        Returns:\n",
    "            String representation of the table\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle different table formats (string or list of lists)\n",
    "            if isinstance(sample['table'], str):\n",
    "                table_data = ast.literal_eval(sample['table'])\n",
    "            else:\n",
    "                table_data = sample['table']\n",
    "            \n",
    "            # Convert to DataFrame and then to a readable string\n",
    "            df = pd.DataFrame(table_data[1:], columns=table_data[0])\n",
    "            return df.to_string(index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting table data: {e}\")\n",
    "            return \"Unable to parse table data\"\n",
    "        \n",
    "    def llm_solve_question(self, sample: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve the question using the LLM with reasoning\n",
    "\n",
    "        Args:\n",
    "            sample (Dict[str, Any]): A singular entry from the dataset\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary with the answer and reasoning\n",
    "        \"\"\"\n",
    "        question = sample.get('qa').get('question')\n",
    "        answer = sample.get('qa').get('answer')\n",
    "        pre_text = sample.get('pre_text')\n",
    "        post_text = sample.get('post_text')\n",
    "        context = f\"Text before the table: {pre_text}\\nText after the table: {post_text}\"\n",
    "        table = self.extract_table_data(sample)\n",
    "        # llm_chain = LLMChain(llm=self.llm,\n",
    "        #                      prompt=self.question_prompt)\n",
    "        # llm_response = llm_chain.run(\n",
    "        #     question=question,\n",
    "        #         context=context,\n",
    "        #         table=table)\n",
    "        # try:\n",
    "        # llm_response = self.llm.invoke(\n",
    "        #     self.question_prompt.format(\n",
    "        #         question=question,\n",
    "        #         context=context,\n",
    "        #         table=table\n",
    "        #     )\n",
    "        # )\n",
    "        llm_response = self.llm(\n",
    "            [\n",
    "                SystemMessage(content='Your role is to answer mathematical questions.'),\n",
    "                HumanMessage(content=self.question_prompt.format(\n",
    "                question=question,\n",
    "                context=context,\n",
    "                table=table\n",
    "                ))\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        llm_response = llm_response.content\n",
    "        llm_response_dict = json.loads(llm_response.replace('\\n', ''))\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"response\": llm_response_dict,\n",
    "            \"actual_answer\": answer\n",
    "        }\n",
    "        \n",
    "        # except Exception as e:\n",
    "        #     return {\n",
    "        #         \"question\": question,\n",
    "        #         \"error\": str(e),\n",
    "        #         \"actual_answer\": answer\n",
    "        #     }\n",
    "            \n",
    "    def validate_answer(self, answer_dict: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Self validation of answer and reasoning\n",
    "\n",
    "        Args:\n",
    "            answer_dict (Dict[str, Any]): Dictionary with answer and reasoning\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: Dictionary of validation\n",
    "        \"\"\"\n",
    "        \n",
    "        question = answer_dict.get('question')\n",
    "        reasoning = answer_dict.get('response')\n",
    "        actual_answer = answer_dict.get('actual_answer')\n",
    "        # llm_chain = LLMChain(llm=self.llm,\n",
    "        #                      prompt=self.validate_prompt)\n",
    "        # llm_response = llm_chain.run(\n",
    "        #         question=question,\n",
    "        #         reasoning=reasoning,\n",
    "        #         actual_answer=actual_answer\n",
    "        #         )\n",
    "        # try:\n",
    "        # llm_response = self.llm.invoke(\n",
    "        #     self.validate_prompt.format(\n",
    "        #         question=question,\n",
    "        #         reasoning=reasoning,\n",
    "        #         actual_answer=actual_answer\n",
    "        #     )\n",
    "        # )\n",
    "        llm_response = self.llm(\n",
    "            [\n",
    "                SystemMessage(content='Your role is to answer mathematical questions.'),\n",
    "                HumanMessage(content=self.validate_prompt.format(\n",
    "                question=question,\n",
    "                reasoning=reasoning,\n",
    "                actual_answer=actual_answer\n",
    "                ))\n",
    "            ]\n",
    "        )\n",
    "        llm_response = llm_response.content\n",
    "        llm_response_dict = json.loads(llm_response.replace('\\n', ''))\n",
    "        answer_dict['validation_response'] = llm_response_dict\n",
    "        return answer_dict\n",
    "        # except Exception as e:\n",
    "        #     answer_dict['validation_response'] = str(e)\n",
    "        #     return answer_dict\n",
    "        \n",
    "    def calculate_accuracy_metrics(self, results: List[Dict[str, Any]]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Calculate accuracy metrics for the financial Q&A results\n",
    "\n",
    "        Args:\n",
    "            results (List[Dict[str, Any]]): List of results from Q&A process\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with accuracy metrics and reasoning\n",
    "        \"\"\"\n",
    "        # Exact Match Accuracy\n",
    "        exact_match = sum(\n",
    "            self._parse_answer(str(result.get('response', {}).get('Final_Answer', ''))) == \n",
    "            self._parse_answer(result.get('actual_answer', ''))\n",
    "            for result in results\n",
    "        ) / len(results)\n",
    "        \n",
    "        # Fuzzy Match Accuracy\n",
    "        fuzzy_match = sum(\n",
    "            self._fuzzy_match(\n",
    "                self._parse_answer(str(result.get('response', {}).get('Final_Answer', ''))), \n",
    "                self._parse_answer(result.get('actual_answer', ''))\n",
    "            )\n",
    "            for result in results\n",
    "        ) / len(results)\n",
    "        \n",
    "        # Validation Confidence\n",
    "        validation_confidence = sum(\n",
    "            result.get('validation_response', {}).get('Validity_Assessment', '') == 'High'\n",
    "            for result in results\n",
    "        ) / len(results)\n",
    "        \n",
    "        return {\n",
    "            \"Exact Match Accuracy\": {\n",
    "                \"value\": exact_match,\n",
    "                \"reasoning\": \"Percentage of answers that match ground truth exactly. Strict metric that requires precise numerical match.\"\n",
    "            },\n",
    "            \"Fuzzy Match Accuracy\": {\n",
    "                \"value\": fuzzy_match,\n",
    "                \"reasoning\": \"Allows for minor numerical variations (e.g., rounding differences). More forgiving than exact match.\"\n",
    "            },\n",
    "            \"Validation Confidence\": {\n",
    "                \"value\": validation_confidence,\n",
    "                \"reasoning\": \"Proportion of solutions marked as 'High' validity by the self-validation mechanism. Indicates internal confidence.\"\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _parse_answer(self, answer: str) -> str:\n",
    "        \"\"\"\n",
    "        Parse and clean numerical answers\n",
    "        \n",
    "        Args:\n",
    "            answer: Raw answer string\n",
    "        \n",
    "        Returns:\n",
    "            Cleaned numerical answer\n",
    "        \"\"\"\n",
    "        # Extract numerical values, handle percentage\n",
    "        matches = re.findall(r'-?\\d+\\.?\\d*', answer)\n",
    "        return matches[0] if matches else ''\n",
    "    \n",
    "    def _fuzzy_match(self, pred: str, truth: str, tolerance: float = 0.1) -> bool:\n",
    "        \"\"\"\n",
    "        Perform fuzzy numerical matching\n",
    "        \n",
    "        Args:\n",
    "            pred: Predicted answer\n",
    "            truth: Ground truth answer\n",
    "            tolerance: Acceptable percentage difference\n",
    "        \n",
    "        Returns:\n",
    "            Boolean indicating if answers are close enough\n",
    "        \"\"\"\n",
    "        try:\n",
    "            pred_float = float(pred)\n",
    "            truth_float = float(truth)         \n",
    "            # Calculate relative difference\n",
    "            diff = abs(pred_float - truth_float) / abs(truth_float)\n",
    "            return diff <= tolerance\n",
    "        except ValueError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as file:\n",
    "    dataset = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_BASE=os.getenv('OPENAI_API_BASE')\n",
    "OPENAI_API_KEY=os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_API_VERSION=os.getenv('OPENAI_API_VERSION')\n",
    "OPENAI_DEPLOYMENT_ID_FC=os.getenv('OPENAI_DEPLOYMENT_ID_FC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=OPENAI_API_BASE,\n",
    "    azure_deployment= OPENAI_DEPLOYMENT_ID_FC,\n",
    "    api_version=OPENAI_API_VERSION,\n",
    "    model_name=OPENAI_DEPLOYMENT_ID_FC,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(huggingfacehub_api_token=token, repo_id=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FinancialQAAgent(llm=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for sample in dataset[:1]:\n",
    "    response = agent.llm_solve_question(sample=sample)\n",
    "    validated_response = agent.validate_answer(answer_dict=response)\n",
    "    results.append(validated_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'what was the percentage change in the net cash from operating activities from 2008 to 2009',\n",
       "  'response': {'Reasoning_Steps': 'To calculate the percentage change in the net cash from operating activities from 2008 to 2009, we need to find the difference between the two values and then divide it by the initial value. Finally, we multiply the result by 100 to get the percentage change.',\n",
       "   'Relevant_Data_Points': ['Net cash from operating activities in 2008: $181001',\n",
       "    'Net cash from operating activities in 2009: $206588'],\n",
       "   'Calculation_Formula': 'Percentage change = ((New value - Old value) / Old value) * 100',\n",
       "   'Potential_Validation_Checks': ['Check the accuracy of the given data points',\n",
       "    'Verify the calculation using alternative methods or sources'],\n",
       "   'Final_Answer': 14.1,\n",
       "   'Confidence_Level': 95},\n",
       "  'actual_answer': '14.1%',\n",
       "  'validation_response': {'Validity_Assessment': 'High',\n",
       "   'Potential_Issues': [],\n",
       "   'Suggested_Improvements': [],\n",
       "   'Confidence_Level': 100}}]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample 1 ---\n",
      "Question: what was the percentage change in the net cash from operating activities from 2008 to 2009\n",
      "Actual Answer: 14.1%\n",
      "Response: {'Reasoning_Steps': 'To calculate the percentage change in the net cash from operating activities from 2008 to 2009, we need to find the difference between the two values and then divide it by the initial value. Finally, we multiply the result by 100 to get the percentage change.', 'Relevant_Data_Points': ['Net cash from operating activities in 2008: $181001', 'Net cash from operating activities in 2009: $206588'], 'Calculation_Formula': 'Percentage change = ((New value - Old value) / Old value) * 100', 'Potential_Validation_Checks': ['Check the accuracy of the given data points', 'Verify the calculation using alternative methods or sources'], 'Final_Answer': 14.1, 'Confidence_Level': 95}\n",
      "Validation Response:: {'Validity_Assessment': 'High', 'Potential_Issues': [], 'Suggested_Improvements': [], 'Confidence_Level': 100}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n--- Sample {i} ---\")\n",
    "    print(f\"Question: {result['question']}\")\n",
    "    print(f\"Actual Answer: {result['actual_answer']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Validation Response:: {result['validation_response']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_float:  14.1\n",
      "truth_float:  14.1\n",
      "diff:  0.0\n"
     ]
    }
   ],
   "source": [
    "metrics = agent.calculate_accuracy_metrics(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exact Match Accuracy': {'value': 1.0,\n",
       "  'reasoning': 'Percentage of answers that match ground truth exactly. Strict metric that requires precise numerical match.'},\n",
       " 'Fuzzy Match Accuracy': {'value': 1.0,\n",
       "  'reasoning': 'Allows for minor numerical variations (e.g., rounding differences). More forgiving than exact match.'},\n",
       " 'Validation Confidence': {'value': 1.0,\n",
       "  'reasoning': \"Proportion of solutions marked as 'High' validity by the self-validation mechanism. Indicates internal confidence.\"}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Accuracy Metrics ---\n",
      "Exact Match Accuracy: 1.0\n",
      "Reasoning: Percentage of answers that match ground truth exactly. Strict metric that requires precise numerical match.\n",
      "\n",
      "Fuzzy Match Accuracy: 0.0\n",
      "Reasoning: Allows for minor numerical variations (e.g., rounding differences). More forgiving than exact match.\n",
      "\n",
      "Validation Confidence: 1.0\n",
      "Reasoning: Proportion of solutions marked as 'High' validity by the self-validation mechanism. Indicates internal confidence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Accuracy Metrics ---\")\n",
    "for metric, details in metrics.items():\n",
    "    print(f\"{metric}: {details['value']}\")\n",
    "    print(f\"Reasoning: {details['reasoning']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qachatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
